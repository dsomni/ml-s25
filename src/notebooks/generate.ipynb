{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from g4f.client import AsyncClient\n",
    "from googletrans import Translator\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor and save tasks dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_TAGS_R = re.compile(\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n",
    "\n",
    "\n",
    "def remove_html_tags(text: str) -> str:\n",
    "    return re.sub(CLEAN_TAGS_R, \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_list(strings: list[str], batch_size: int) -> list[list[str]]:\n",
    "    return [strings[i : i + batch_size] for i in range(0, len(strings), batch_size)]\n",
    "\n",
    "\n",
    "async def translate_text(text: str, src: str = \"ru\", dest: str = \"en\") -> str:\n",
    "    translator = Translator()\n",
    "    translation = await translator.translate(text, src=src, dest=dest)\n",
    "    return translation.text\n",
    "\n",
    "\n",
    "async def translate_texts_inner(\n",
    "    texts: list[str], src: str = \"ru\", dest: str = \"en\"\n",
    ") -> list[str]:\n",
    "    translator = Translator()\n",
    "    if len(texts) > 1:\n",
    "        return [res.text for res in await translator.translate(texts, src=src, dest=dest)]\n",
    "    res = await translator.translate(texts[0], src=src, dest=dest)\n",
    "    return [res.text]\n",
    "\n",
    "\n",
    "async def translate_texts(texts: list[str]) -> list[str]:\n",
    "    res = await tqdm_asyncio.gather(\n",
    "        *[translate_texts_inner(text_list) for text_list in batch_list(texts, 5)]\n",
    "    )\n",
    "\n",
    "    ans = []\n",
    "    for r in res:\n",
    "        ans.extend(r)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(tasks_dict: dict, path: str):\n",
    "    with open(path, \"w\", encoding=\"utf8\") as f:\n",
    "        json.dump(tasks_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "def build_task_str(task: dict) -> str:\n",
    "    return remove_html_tags(\n",
    "        \"\"\n",
    "        # f\"{task['title']}\\n\\n\"\n",
    "        # + f\"Теги:{task['tags']}\\n\\n\"\n",
    "        + f\"{task['description']}\\n\\n\"\n",
    "        + f\"Формат входных данных:\\n{task['inputFormat']}\\n\\n\"\n",
    "        + f\"Формат выходных данных:\\n{task['outputFormat']}\\n\\n\"\n",
    "        # + f\"Примеры\\n{task['examples']}\"\n",
    "        # + (f\"\\n\\nПримечание:\\n{task['remark']}\" if task[\"remark\"] != \"\" else \"\")\n",
    "    )\n",
    "\n",
    "\n",
    "def compile_tasks_dict() -> dict[str, str]:\n",
    "    df = pd.read_csv(\"../../data/db_tasks.csv\")\n",
    "    raw_dict = df.fillna(\"\").set_index(\"spec\").to_dict(orient=\"index\")\n",
    "\n",
    "    return {str(spec): build_task_str(task) for spec, task in raw_dict.items()}\n",
    "\n",
    "\n",
    "async def save_dicts():\n",
    "    tasks_dict = compile_tasks_dict()\n",
    "    save_dict(tasks_dict, \"../../data/generated/db_tasks_ru.json\")\n",
    "\n",
    "    specs, values = zip(*tasks_dict.items())\n",
    "    translated_values = await translate_texts(list(values))\n",
    "    tasks_dict_en = dict(zip(specs, translated_values))\n",
    "    save_dict(tasks_dict_en, \"../../data/generated/db_tasks_en.json\")\n",
    "\n",
    "\n",
    "# await save_dicts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tasks dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "553"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_dict(path: str) -> dict:\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "\n",
    "\n",
    "tasks_dict: dict[str, str] = load_dict(\"../../data/generated/db_tasks_en.json\")\n",
    "len(tasks_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAILABLE_MODELS = [\n",
    "    # \"gpt-3.5-turbo\",\n",
    "    # \"gpt-4\",\n",
    "    # \"gpt-4o\",\n",
    "    # \"gpt-4o-mini\",\n",
    "    # \"o1\",\n",
    "    \"o1-mini\",\n",
    "    \"o3-mini\",\n",
    "    # \"gigachat\",\n",
    "    # \"meta-ai\",\n",
    "    \"llama-2-7b\",\n",
    "    \"llama-3-8b\",\n",
    "    \"llama-3-70b\",\n",
    "    \"llama-3.1-8b\",\n",
    "    \"llama-3.1-70b\",\n",
    "    # \"llama-3.1-405b\",\n",
    "    \"llama-3.2-1b\",\n",
    "    \"llama-3.2-3b\",\n",
    "    \"llama-3.2-11b\",\n",
    "    # \"llama-3.2-90b\",\n",
    "    \"llama-3.3-70b\",\n",
    "    \"mixtral-8x7b\",\n",
    "    \"mixtral-8x22b\",\n",
    "    \"mistral-nemo\",\n",
    "    \"mixtral-small-24b\",\n",
    "    # \"hermes-3\",\n",
    "    \"phi-3.5-mini\",\n",
    "    \"phi-4\",\n",
    "    # \"wizardlm-2-7b\",\n",
    "    # \"wizardlm-2-8x22b\",\n",
    "    \"gemini-exp\",\n",
    "    \"gemini-1.5-flash\",\n",
    "    \"gemini-1.5-pro\",\n",
    "    \"gemini-2.0\",\n",
    "    # \"gemini-2.0-flash\",\n",
    "    # \"gemini-2.0-flash-thinking\",\n",
    "    # \"gemini-2.0-pro\",\n",
    "    # \"claude-3-haiku\",\n",
    "    # \"claude-3-sonnet\",\n",
    "    # \"claude-3-opus\",\n",
    "    # \"claude-3.5-sonnet\",\n",
    "    # \"claude-3.7-sonnet\",\n",
    "    # \"claude-3.7-sonnet-thinking\",\n",
    "    \"reka-core\",\n",
    "    \"blackboxai\",\n",
    "    \"blackboxai-pro\",\n",
    "    \"command-r\",\n",
    "    \"command-r-plus\",\n",
    "    \"command-r7b\",\n",
    "    \"command-a\",\n",
    "    # \"qwen-1.5-7b\",\n",
    "    \"qwen-2-72b\",\n",
    "    \"qwen-2-vl-7b\",\n",
    "    \"qwen-2.5-72b\",\n",
    "    \"qwen-2.5-coder-32b\",\n",
    "    \"qwen-2.5-1m\",\n",
    "    \"qwen-2-5-max\",\n",
    "    \"qwq-32b\",\n",
    "    \"qvq-72b\",\n",
    "    \"pi\",\n",
    "    \"deepseek-chat\",\n",
    "    \"deepseek-v3\",\n",
    "    # \"deepseek-r1\",\n",
    "    # \"janus-pro-7b\",\n",
    "    # \"grok-3\",\n",
    "    # \"grok-3-r1\",\n",
    "    # \"sonar\",\n",
    "    # \"sonar-pro\",\n",
    "    # \"sonar-reasoning\",\n",
    "    # \"sonar-reasoning-pro\",\n",
    "    # \"r1-1776\",\n",
    "    \"nemotron-70b\",\n",
    "    # \"dbrx-instruct\",\n",
    "    # \"glm-4\",\n",
    "    # \"yi-34b\",\n",
    "    # \"dolphin-2.6\",\n",
    "    # \"dolphin-2.9\",\n",
    "    # \"airoboros-70b\",\n",
    "    # \"lzlv-70b\",\n",
    "    # \"minicpm-2.5\",\n",
    "    # \"tulu-3-1-8b\",\n",
    "    # \"tulu-3-70b\",\n",
    "    # \"tulu-3-405b\",\n",
    "    # \"olmo-1-7b\",\n",
    "    \"olmo-2-13b\",\n",
    "    \"olmo-2-32b\",\n",
    "    \"olmo-4-synthetic\",\n",
    "    \"lfm-40b\",\n",
    "    \"evil\",\n",
    "]\n",
    "\n",
    "AVAILABLE_MODELS = [\n",
    "    \"evil\",\n",
    "    # \"llama-3.1-8b\",\n",
    "    \"llama-3.2-3b\",\n",
    "    \"blackboxai\",\n",
    "    \"blackboxai-pro\",\n",
    "    \"deepseek-chat\",\n",
    "    \"deepseek-v3\",\n",
    "]\n",
    "\n",
    "\n",
    "def PROMPT(task):\n",
    "    return f\"\"\"Write a Python solution for the following task.\n",
    "The code should look like it was written by an intermediate student: practical but not overly optimized or perfect. Follow these guidelines:\n",
    "1. Use not overly long names (e.g., res instead of result or final_output_value).\n",
    "2. Do not include comments or explanations\n",
    "3. Avoid using functions, prefer straightforward logic\n",
    "4. Apply small stylistic deviations, like mixing single/double quotes, occasional redundant logic, inconsistent spacing, etc\n",
    "5. No error handling\n",
    "6. Do not print to output anything except answer to the problem without any annotations\n",
    " Finally, return just pure python code included in ```python ``` section\n",
    "Task: \\n{task}.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_python_code(res: str) -> str:\n",
    "    return (\n",
    "        re.findall(r\"```python(.*?)```\", res, re.DOTALL)[0]\n",
    "        .encode(\"utf-8\", errors=\"ignore\")\n",
    "        .decode(\"utf-8\")\n",
    "    )\n",
    "\n",
    "\n",
    "async def access_llm(\n",
    "    client: AsyncClient, model: str, prompt: str\n",
    ") -> tuple[Optional[str], str, str]:\n",
    "    try:\n",
    "        response = await client.chat.completions.create(\n",
    "            model=model, messages=[{\"role\": \"user\", \"content\": prompt}], web_search=False\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return None, model, str(e)\n",
    "    res = \"\"\n",
    "    try:\n",
    "        res = response.choices[0].message.content\n",
    "        return extract_python_code(response.choices[0].message.content), model, \"\"\n",
    "    except Exception as e2:\n",
    "        return None, model, str(e2) + (f\"\\n{res}\" if len(res) > 0 else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_COLUMNS = [\"task\", \"code\", \"model\"]\n",
    "ERR_COLUMNS = [\"task\", \"model\", \"error\"]\n",
    "\n",
    "\n",
    "def setup_output(path: str, columns: list[str] = OUTPUT_COLUMNS):\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        csv.DictWriter(f, fieldnames=columns).writeheader()\n",
    "\n",
    "\n",
    "def append_output(path: str, data: dict, columns: list[str] = OUTPUT_COLUMNS):\n",
    "    with open(path, \"a\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=columns)\n",
    "        writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_solutions(\n",
    "    task_dict: dict,\n",
    "    output: str,\n",
    "    output_err: str,\n",
    "    prompt=PROMPT,\n",
    "    models: list[str] = AVAILABLE_MODELS,\n",
    "    timeout: float = 60.0,\n",
    ") -> list:\n",
    "    if not os.path.exists(output):\n",
    "        setup_output(output, columns=OUTPUT_COLUMNS)\n",
    "    if not os.path.exists(output_err):\n",
    "        setup_output(output_err, columns=ERR_COLUMNS)\n",
    "\n",
    "    solutions = []\n",
    "\n",
    "    for spec, task in tqdm.tqdm(task_dict.items()):\n",
    "        client = AsyncClient()\n",
    "\n",
    "        results = [access_llm(client, m, prompt(task)) for m in models]\n",
    "\n",
    "        for coro in asyncio.as_completed(results):\n",
    "            try:\n",
    "                solution, m, err = await asyncio.wait_for(coro, timeout)\n",
    "                if solution is None or len(solution) == 0:\n",
    "                    append_output(\n",
    "                        output_err,\n",
    "                        {\"task\": spec, \"model\": m, \"error\": err},\n",
    "                        columns=ERR_COLUMNS,\n",
    "                    )\n",
    "                    continue\n",
    "                solutions.append((spec, m, solution))\n",
    "                append_output(\n",
    "                    output,\n",
    "                    {\"task\": spec, \"code\": solution, \"model\": m},\n",
    "                    columns=OUTPUT_COLUMNS,\n",
    "                )\n",
    "            except asyncio.TimeoutError:\n",
    "                continue\n",
    "            except BaseException:\n",
    "                continue\n",
    "        await asyncio.sleep(2)\n",
    "\n",
    "    return solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_dict = {}\n",
    "task_specs = list(tasks_dict.keys())\n",
    "idx = task_specs.index(\"5bab2114-725a-4548-aa0e-c6b7296898d5\")\n",
    "pass_dict = {k: tasks_dict[k] for k in task_specs[idx + 1 :]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/85 [00:06<08:38,  6.17s/it]Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x000001AB83917380>\n",
      "Unclosed connector\n",
      "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001AB83544770>, 41724.015)])']\n",
      "connector: <aiohttp.connector.TCPConnector object at 0x000001ABFFF7B710>\n",
      " 18%|█▊        | 15/85 [03:22<15:52, 13.60s/it]Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x000001AB836765D0>\n",
      "Unclosed connector\n",
      "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001AB838B6870>, 41920.468)])']\n",
      "connector: <aiohttp.connector.TCPConnector object at 0x000001AB83676D50>\n",
      " 32%|███▏      | 27/85 [04:57<07:38,  7.91s/it]Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x000001ABFFEF9EE0>\n",
      "Unclosed connector\n",
      "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001AB83966210>, 42015.812)])']\n",
      "connector: <aiohttp.connector.TCPConnector object at 0x000001AB82008140>\n",
      " 80%|████████  | 68/85 [16:39<03:08, 11.10s/it]Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x000001AB82297F50>\n",
      "Unclosed connector\n",
      "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001AB838B5490>, 42716.89)])']\n",
      "connector: <aiohttp.connector.TCPConnector object at 0x000001ABFFEF8710>\n",
      " 93%|█████████▎| 79/85 [22:19<02:21, 23.55s/it]Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x000001ABFFF7ADB0>\n",
      "Unclosed connector\n",
      "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001AB838B7710>, 43055.687)])']\n",
      "connector: <aiohttp.connector.TCPConnector object at 0x000001ABFFF79400>\n",
      "100%|██████████| 85/85 [23:00<00:00, 16.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# pass_dict = {\n",
    "#     k: tasks_dict[k]\n",
    "#     for k in [\n",
    "#         \"c0df7d49-26f5-451c-b44a-1e0bca60bca5\",\n",
    "#         \"4e5b21c0-e86f-4eac-82b6-1a0d00ae4199\",\n",
    "#     ]\n",
    "# }\n",
    "# pass_dict  = tasks_dict\n",
    "\n",
    "# solutions = await generate_solutions(pass_dict, \"test.csv\", \"test_log.csv\", timeout=5)\n",
    "solutions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solutions = 483\n",
      "Possible solutions = 510\n"
     ]
    }
   ],
   "source": [
    "print(f\"Solutions = {len(solutions)}\")\n",
    "print(f\"Possible solutions = {len(pass_dict) * len(AVAILABLE_MODELS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3177"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df = pd.read_csv(\"results.csv\")\n",
    "len(res_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
