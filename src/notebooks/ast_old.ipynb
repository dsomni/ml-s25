{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b6999c",
   "metadata": {},
   "source": [
    "# Solution based on AST (on old dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01c8a306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tree_sitter_python as tspython\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from tree_sitter import Language, Parser\n",
    "\n",
    "PY_LANGUAGE = Language(tspython.language())\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580a6318",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55860f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 420):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978ce47c",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb97800f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size: 9251\n",
      "\n",
      "Train size: 6475\n",
      "Validation size: 1943\n",
      "Test size: 833\n"
     ]
    }
   ],
   "source": [
    "set_seed()\n",
    "df = pd.read_csv(\"../../data/generated/dataset_old.csv\")\n",
    "\n",
    "print(f\"Total size: {len(df)}\\n\")\n",
    "\n",
    "train_df, val_prep = train_test_split(df, test_size=0.3, stratify=df[\"generated\"])\n",
    "valid_df, test_df = train_test_split(\n",
    "    val_prep, test_size=0.3, stratify=val_prep[\"generated\"]\n",
    ")\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "valid_df = valid_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Validation size: {len(valid_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2d8a9d",
   "metadata": {},
   "source": [
    "Check that the data is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d43e53df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean generated (train): 0.3567567567567568\n",
      "Mean generated (validation): 0.3566649511065363\n",
      "Mean generated (test): 0.3565426170468187\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean generated (train): {train_df['generated'].mean()}\")\n",
    "print(f\"Mean generated (validation): {valid_df['generated'].mean()}\")\n",
    "print(f\"Mean generated (test): {test_df['generated'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066cb0c6",
   "metadata": {},
   "source": [
    "## Dataset building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f4e1a40ef64f9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Parser(PY_LANGUAGE)\n",
    "node_types = set()\n",
    "\n",
    "\n",
    "def walk_tree(node, types):\n",
    "    types.append(node.type)\n",
    "    for child in node.children:\n",
    "        walk_tree(child, types)\n",
    "\n",
    "\n",
    "def code_to_feature_vector(code: bytes, device=DEVICE) -> torch.Tensor:\n",
    "    tree = parser.parse(code)\n",
    "    types = []\n",
    "    walk_tree(tree.root_node, types)\n",
    "    counts = Counter(types)\n",
    "    feature_vector = [counts.get(typ, 0) for typ in node_types]\n",
    "    return torch.tensor(feature_vector, dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "# Gather all node types\n",
    "for _, row in train_df.iterrows():\n",
    "    tree = parser.parse(str.encode(row[\"code\"]))\n",
    "    types = []\n",
    "    walk_tree(tree.root_node, types)\n",
    "    node_types.update(types)\n",
    "\n",
    "node_types = sorted(node_types)\n",
    "type_to_idx = {typ: i for i, typ in enumerate(node_types)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7c9788",
   "metadata": {},
   "source": [
    "Save node types for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1db913e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/ast/node_types_old.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(node_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6474104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASTDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[str, torch.Tensor, float]:\n",
    "        raw_code = self.dataframe[\"code\"].iloc[index]\n",
    "        return (\n",
    "            raw_code,\n",
    "            code_to_feature_vector(raw_code.encode(\"utf-8\")),\n",
    "            float(self.dataframe[\"generated\"].iloc[index]),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "\n",
    "data_train = ASTDataset(dataframe=train_df)\n",
    "dataloader_train = DataLoader(data_train, batch_size=32)\n",
    "data_val = ASTDataset(dataframe=valid_df)\n",
    "dataloader_val = DataLoader(data_val, batch_size=128)\n",
    "data_test = ASTDataset(dataframe=test_df)\n",
    "dataloader_test = DataLoader(data_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8911781",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c390f5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIDetector(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 32):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32630637",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1dde22ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions: list[float], labels: list[float]) -> dict:\n",
    "    predictions_rounded = [round(x) for x in predictions]\n",
    "    labels_rounded = [round(x) for x in labels]\n",
    "\n",
    "    return {\n",
    "        \"recall\": recall_score(labels_rounded, predictions_rounded),\n",
    "        \"roc_auc\": roc_auc_score(labels_rounded, predictions_rounded),\n",
    "        \"f1\": f1_score(labels_rounded, predictions_rounded),\n",
    "        \"mae\": mean_absolute_error(labels, predictions),\n",
    "        \"mse\": mean_squared_error(labels, predictions),\n",
    "    }\n",
    "\n",
    "\n",
    "def metrics_str(metrics: dict) -> str:\n",
    "    return \" | \".join([f\"{key.upper()}: {value:.4f}\" for key, value in metrics.items()])\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module, dataloader: DataLoader, criterion, optimizer: optim.Optimizer\n",
    "):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for _, code, label in tqdm(dataloader, desc=\"Training\"):\n",
    "        code, label = code.float().to(DEVICE), label.float().to(DEVICE)\n",
    "        outputs = model(code)\n",
    "        outputs = outputs.squeeze()\n",
    "        loss = criterion(outputs, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, dataloader: DataLoader) -> dict:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_predictions = []\n",
    "        all_truths = []\n",
    "        for real_code, code, label in tqdm(dataloader, desc=\"Validation\"):\n",
    "            code, label = code.float().to(DEVICE), label.float().to(DEVICE)\n",
    "            outputs = model(code)\n",
    "            outputs = outputs.squeeze()\n",
    "\n",
    "            all_predictions.extend(outputs.detach().cpu().numpy().tolist())\n",
    "            all_truths.extend(label.detach().cpu().numpy().tolist())\n",
    "\n",
    "        return compute_metrics(all_predictions, all_truths)\n",
    "\n",
    "\n",
    "def train_eval_loop(\n",
    "    model: nn.Module,\n",
    "    dataloader_train: DataLoader,\n",
    "    dataloader_val: DataLoader,\n",
    "    criterion,\n",
    "    optimizer: optim.Optimizer,\n",
    "    epochs: int = 5,\n",
    "    early_stopping: int = 3,\n",
    "    maximize: str = \"recall\",\n",
    "    save_path: str = \"../../data/ast/best_model_old.pth\",\n",
    "):\n",
    "    best_score = 0 if maximize == \"recall\" else float(\"inf\")\n",
    "    no_improvement = 0\n",
    "    mean_losses = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        mean_loss = train_model(model, dataloader_train, criterion, optimizer)\n",
    "        metrics = evaluate_model(model, dataloader_val)\n",
    "\n",
    "        mean_losses.append(mean_loss)\n",
    "        print(f\"\\n{metrics_str(metrics)}\\n\")\n",
    "\n",
    "        score = metrics[maximize]\n",
    "        if (maximize == \"recall\" and score > best_score) or (\n",
    "            maximize == \"mae\" and score < best_score\n",
    "        ):\n",
    "            no_improvement = 0\n",
    "            best_score = score\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "        if no_improvement >= early_stopping:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    return mean_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa4aa79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [00:05<00:00, 35.39it/s]\n",
      "Validation: 100%|██████████| 16/16 [00:01<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RECALL: 0.4545 | ROC_AUC: 0.6921 | F1: 0.5748 | MAE: 0.2554 | MSE: 0.2126\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [00:06<00:00, 30.90it/s]\n",
      "Validation: 100%|██████████| 16/16 [00:01<00:00,  9.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RECALL: 0.5094 | ROC_AUC: 0.7091 | F1: 0.6086 | MAE: 0.2435 | MSE: 0.1991\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [00:07<00:00, 28.85it/s]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RECALL: 0.5830 | ROC_AUC: 0.7099 | F1: 0.6211 | MAE: 0.2683 | MSE: 0.2149\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [00:06<00:00, 29.98it/s]\n",
      "Validation: 100%|██████████| 16/16 [00:01<00:00,  9.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RECALL: 0.6205 | ROC_AUC: 0.7202 | F1: 0.6380 | MAE: 0.2660 | MSE: 0.2082\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [00:07<00:00, 28.40it/s]\n",
      "Validation: 100%|██████████| 16/16 [00:01<00:00, 12.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RECALL: 0.5325 | ROC_AUC: 0.7134 | F1: 0.6181 | MAE: 0.2449 | MSE: 0.1973\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [00:05<00:00, 34.46it/s]\n",
      "Validation: 100%|██████████| 16/16 [00:01<00:00,  8.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RECALL: 0.5945 | ROC_AUC: 0.7213 | F1: 0.6363 | MAE: 0.2506 | MSE: 0.2025\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [00:06<00:00, 33.73it/s]\n",
      "Validation: 100%|██████████| 16/16 [00:01<00:00, 10.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RECALL: 0.5440 | ROC_AUC: 0.7272 | F1: 0.6379 | MAE: 0.2280 | MSE: 0.1902\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [00:06<00:00, 33.42it/s]\n",
      "Validation: 100%|██████████| 16/16 [00:01<00:00, 10.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RECALL: 0.4589 | ROC_AUC: 0.6910 | F1: 0.5745 | MAE: 0.2468 | MSE: 0.2121\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [00:05<00:00, 37.20it/s]\n",
      "Validation: 100%|██████████| 16/16 [00:01<00:00,  8.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RECALL: 0.7388 | ROC_AUC: 0.6990 | F1: 0.6278 | MAE: 0.3237 | MSE: 0.2490\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [00:06<00:00, 32.30it/s]\n",
      "Validation: 100%|██████████| 16/16 [00:01<00:00, 10.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RECALL: 0.7403 | ROC_AUC: 0.6813 | F1: 0.6114 | MAE: 0.3356 | MSE: 0.2752\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "set_seed()\n",
    "model = AIDetector(input_dim=len(node_types)).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "losses = train_eval_loop(\n",
    "    model,\n",
    "    dataloader_train,\n",
    "    dataloader_val,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    epochs=10,\n",
    "    early_stopping=5,\n",
    "    maximize=\"recall\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360558b2",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83b7bea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model: nn.Module, dataloader: DataLoader) -> tuple[pd.DataFrame, dict]:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_predictions = []\n",
    "        all_truths = []\n",
    "        all_codes = []\n",
    "        for real_code, code, label in tqdm(dataloader, desc=\"Validation\"):\n",
    "            code, label = code.float().to(DEVICE), label.float().to(DEVICE)\n",
    "            outputs = model(code)\n",
    "            outputs = outputs.squeeze()\n",
    "\n",
    "            all_predictions.extend(outputs.detach().cpu().numpy().tolist())\n",
    "            all_truths.extend(label.detach().cpu().numpy().tolist())\n",
    "            all_codes.extend(real_code)\n",
    "\n",
    "        test_df = pd.DataFrame(\n",
    "            {\"code\": all_codes, \"real\": all_truths, \"predicted\": all_predictions}\n",
    "        )\n",
    "        return test_df, compute_metrics(all_predictions, all_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bad998ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 7/7 [00:00<00:00, 10.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECALL: 0.7172 | ROC_AUC: 0.6459 | F1: 0.5772 | MAE: 0.3725 | MSE: 0.3189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_model = AIDetector(input_dim=len(node_types)).to(DEVICE)\n",
    "best_model.load_state_dict(torch.load(\"../../data/ast/best_model_old.pth\"))\n",
    "test_df, test_metrics = test_model(best_model, dataloader_test)\n",
    "print(metrics_str(test_metrics))\n",
    "\n",
    "test_df.to_csv(\"../../data/ast/test_results_old.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba93e1",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e5fa4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_ai_code(code: str) -> float:\n",
    "    with open(\"../../data/ast/node_types_old.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        node_types_loaded = f.readlines()\n",
    "\n",
    "    loaded_model = AIDetector(input_dim=len(node_types_loaded))\n",
    "    loaded_model.load_state_dict(torch.load(\"../../data/ast/best_model_old.pth\"))\n",
    "\n",
    "    code_vectorized = code_to_feature_vector(\n",
    "        code.encode(\"utf-8\"), device=torch.device(\"cpu\")\n",
    "    ).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        prediction = loaded_model(code_vectorized).squeeze().cpu().item()\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a29df5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************\n",
      "CODE:\n",
      "\n",
      "a,b = map(int, input().split())\n",
      "if a > b:\n",
      "    return 1\n",
      "return 0\n",
      "\n",
      "PREDICTION: 0.9982\n",
      "\n",
      "***************\n",
      "CODE:\n",
      "\n",
      "x, y = map(int, input().split())\n",
      "return int(x>y)\n",
      "\n",
      "PREDICTION: 0.9855\n",
      "\n",
      "***************\n",
      "CODE:\n",
      "\n",
      "l = map(int, input().split())\n",
      "if l[0] > l[1] :\n",
      "    return 1\n",
      "else:\n",
      "    return 0\n",
      "\n",
      "PREDICTION: 0.9903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "code1 = \"\"\"\n",
    "a,b = map(int, input().split())\n",
    "if a > b:\n",
    "    return 1\n",
    "return 0\n",
    "\"\"\"\n",
    "\n",
    "code2 = \"\"\"\n",
    "x, y = map(int, input().split())\n",
    "return int(x>y)\n",
    "\"\"\"\n",
    "\n",
    "code3 = \"\"\"\n",
    "l = map(int, input().split())\n",
    "if l[0] > l[1] :\n",
    "    return 1\n",
    "else:\n",
    "    return 0\n",
    "\"\"\"\n",
    "for c in [code1, code2, code3]:\n",
    "    print(f\"{'*' * 15}\\nCODE:\\n{c}\\nPREDICTION: {detect_ai_code(c):.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
